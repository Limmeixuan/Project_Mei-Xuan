# Data for Accuracy, Precision, Recall, and F1 Score across different algorithms
accuracy_data = [0.8478260869565217, 0.8315217391304348, 0.8315217391304348, 0.8369565217391305, 0.8369565217391305, 0.8369565217391305, 0.8260869565217391, 0.8369565217391305, 0.842391304347826, 0.8478260869565217]
precision_data = [0.8910891089108911, 0.8877551020408163, 0.88, 0.8811881188118812, 0.8969072164948454, 0.8811881188118812, 0.8712871287128713, 0.8969072164948454, 0.8979591836734694, 0.9072164948453608]
recall_data = [0.8411214953271028, 0.8130841121495327, 0.822429906542056, 0.8317757009345794, 0.8130841121495327, 0.8317757009345794, 0.822429906542056, 0.8130841121495327, 0.822429906542056, 0.822429906542056]
f1_score_data = [0.8653846153846154, 0.848780487804878, 0.8502415458937198, 0.8557692307692306, 0.8529411764705883, 0.8557692307692306, 0.8461538461538461, 0.8529411764705883, 0.8585365853658536, 0.8627450980392157] 

# Calculate the average for each metric
average_accuracy = sum(accuracy_data) / len(accuracy_data)
average_precision = sum(precision_data) / len(precision_data)
average_recall = sum(recall_data) / len(recall_data)
average_f1_score = sum(f1_score_data) / len(f1_score_data)

print("Average Accuracy:", average_accuracy)
print("Average Precision:", average_precision)
print("Average Recall:", average_recall)
print("Average F1 Score:", average_f1_score)
