# Data for Accuracy, Precision, Recall, and F1 Score across different algorithms
accuracy_data = [0.7880434782608695, 0.782608695652174, 0.8206521739130435, 0.8097826086956522, 0.7880434782608695, 0.8097826086956522, 0.8097826086956522, 0.8043478260869565, 0.8097826086956522, 0.8097826086956522]
precision_data = [0.8695652173913043, 0.8602150537634409, 0.8775510204081632, 0.875, 0.8617021276595744, 0.875, 0.8673469387755102,0.865979381443299, 0.875, 0.8673469387755102]
recall_data = [0.7476635514018691, 0.7476635514018691, 0.8037383177570093, 0.7850467289719626, 0.7570093457943925, 0.7850467289719626, 0.794392523364486, 0.7850467289719626, 0.7850467289719626, 0.794392523364486]
f1_score_data = [0.8040201005025125, 0.7999999999999999, 0.8390243902439024, 0.8275862068965517, 0.8059701492537313, 0.8275862068965517, 0.8292682926829268, 0.8235294117647058, 0.8275862068965517, 0.8292682926829268]

# Calculate the average for each metric
average_accuracy = sum(accuracy_data) / len(accuracy_data)
average_precision = sum(precision_data) / len(precision_data)
average_recall = sum(recall_data) / len(recall_data)
average_f1_score = sum(f1_score_data) / len(f1_score_data)

print("Average Accuracy:", average_accuracy)
print("Average Precision:", average_precision)
print("Average Recall:", average_recall)
print("Average F1 Score:", average_f1_score)
