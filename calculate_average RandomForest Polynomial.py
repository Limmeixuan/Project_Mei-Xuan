# Data for Accuracy, Precision, Recall, and F1 Score across different algorithms
accuracy_data = [0.8913043478260869, 0.8858695652173914, 0.8913043478260869, 0.8967391304347826, 0.8967391304347826, 0.8967391304347826, 0.8967391304347826, 0.875, 0.8967391304347826, 0.907608695652174]
precision_data = [0.9223300970873787, 0.9215686274509803, 0.9142857142857143, 0.9230769230769231, 0.9313725490196079, 0.9230769230769231, 0.9074074074074074, 0.8888888888888888, 0.9230769230769231, 0.9326923076923077]
recall_data = [0.8878504672897196, 0.8785046728971962, 0.897196261682243, 0.897196261682243, 0.8878504672897196, 0.897196261682243, 0.9158878504672897, 0.897196261682243, 0.897196261682243, 0.9065420560747663]
f1_score_data = [0.9047619047619048, 0.8995215311004785, 0.9056603773584906, 0.9099526066350712,  0.9090909090909091, 0.9099526066350712, 0.9116279069767442, 0.8930232558139535, 0.9099526066350712, 0.919431279620853]

# Calculate the average for each metric
average_accuracy = sum(accuracy_data) / len(accuracy_data)
average_precision = sum(precision_data) / len(precision_data)
average_recall = sum(recall_data) / len(recall_data)
average_f1_score = sum(f1_score_data) / len(f1_score_data)

print("Average Accuracy:", average_accuracy)
print("Average Precision:", average_precision)
print("Average Recall:", average_recall)
print("Average F1 Score:", average_f1_score)
