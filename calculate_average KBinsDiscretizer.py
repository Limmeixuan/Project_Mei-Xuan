# Data for Accuracy, Precision, Recall, and F1 Score across different algorithms
accuracy_data = [0.8804347826086957, 0.8804347826086957, 0.8804347826086957, 0.8804347826086957, 0.8804347826086957, 0.8804347826086957, 0.8804347826086957, 0.8804347826086957, 0.8804347826086957, 0.8804347826086957]
precision_data = [0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048]
recall_data = [0.8878504672897196, 0.8878504672897196, 0.8878504672897196, 0.8878504672897196, 0.8878504672897196, 0.8878504672897196, 0.8878504672897196, 0.8878504672897196, 0.8878504672897196, 0.8878504672897196]
f1_score_data = [0.8962264150943396, 0.8962264150943396, 0.8962264150943396, 0.8962264150943396, 0.8962264150943396, 0.8962264150943396, 0.8962264150943396, 0.8962264150943396, 0.8962264150943396, 0.8962264150943396]

# Calculate the average for each metric
average_accuracy = sum(accuracy_data) / len(accuracy_data)
average_precision = sum(precision_data) / len(precision_data)
average_recall = sum(recall_data) / len(recall_data)
average_f1_score = sum(f1_score_data) / len(f1_score_data)

print("Average Accuracy:", average_accuracy)
print("Average Precision:", average_precision)
print("Average Recall:", average_recall)
print("Average F1 Score:", average_f1_score)
