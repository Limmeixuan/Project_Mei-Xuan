# Data for Accuracy, Precision, Recall, and F1 Score across different algorithms
accuracy_data = [0.8913043478260869, 0.8913043478260869, 0.8913043478260869, 0.8913043478260869, 0.8913043478260869, 0.8913043478260869, 0.8913043478260869, 0.8913043478260869, 0.8913043478260869, 0.8913043478260869]
precision_data = [0.9142857142857143, 0.9142857142857143, 0.9142857142857143, 0.9142857142857143, 0.9142857142857143, 0.9142857142857143, 0.9142857142857143, 0.9142857142857143, 0.9142857142857143, 0.9142857142857143]
recall_data = [0.897196261682243, 0.897196261682243, 0.897196261682243, 0.897196261682243, 0.897196261682243, 0.897196261682243, 0.897196261682243, 0.897196261682243, 0.897196261682243, 0.897196261682243]
f1_score_data = [0.9056603773584906, 0.9056603773584906, 0.9056603773584906, 0.9056603773584906, 0.9056603773584906, 0.9056603773584906, 0.9056603773584906, 0.9056603773584906, 0.9056603773584906, 0.9056603773584906]

# Calculate the average for each metric
average_accuracy = sum(accuracy_data) / len(accuracy_data)
average_precision = sum(precision_data) / len(precision_data)
average_recall = sum(recall_data) / len(recall_data)
average_f1_score = sum(f1_score_data) / len(f1_score_data)

print("Average Accuracy:", average_accuracy)
print("Average Precision:", average_precision)
print("Average Recall:", average_recall)
print("Average F1 Score:", average_f1_score)
