# Data for Accuracy, Precision, Recall, and F1 Score across different algorithms
accuracy_data = [0.8532608695652174, 0.8532608695652174, 0.8532608695652174, 0.8532608695652174, 0.8532608695652174, 0.8532608695652174, 0.8532608695652174, 0.8532608695652174, 0.8532608695652174, 0.8532608695652174]
precision_data = [0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9]
recall_data = [0.8411214953271028, 0.8411214953271028, 0.8411214953271028, 0.8411214953271028, 0.8411214953271028, 0.8411214953271028, 0.8411214953271028, 0.8411214953271028, 0.8411214953271028, 0.8411214953271028]
f1_score_data = [0.8695652173913043, 0.8695652173913043, 0.8695652173913043, 0.8695652173913043, 0.8695652173913043, 0.8695652173913043, 0.8695652173913043, 0.8695652173913043, 0.8695652173913043, 0.8695652173913043] 

# Calculate the average for each metric
average_accuracy = sum(accuracy_data) / len(accuracy_data)
average_precision = sum(precision_data) / len(precision_data)
average_recall = sum(recall_data) / len(recall_data)
average_f1_score = sum(f1_score_data) / len(f1_score_data)

print("Average Accuracy:", average_accuracy)
print("Average Precision:", average_precision)
print("Average Recall:", average_recall)
print("Average F1 Score:", average_f1_score)

# Binaing or Discrelization Data for Accuracy, Precision, Recall, and F1 Score across different algorithms
accuracy_data = [0.8043478260869565, 0.8043478260869565, 0.8043478260869565, 0.8043478260869565, 0.8043478260869565, 0.8043478260869565, 0.8043478260869565, 0.8043478260869565, 0.8043478260869565, 0.8043478260869565]
precision_data = [0.8901098901098901, 0.8901098901098901, 0.8901098901098901, 0.8901098901098901, 0.8901098901098901, 0.8901098901098901, 0.8901098901098901, 0.8901098901098901, 0.8901098901098901, 0.8901098901098901] 
recall_data = [0.7570093457943925, 0.7570093457943925, 0.7570093457943925, 0.7570093457943925, 0.7570093457943925, 0.7570093457943925, 0.7570093457943925, 0.7570093457943925, 0.7570093457943925,0.7570093457943925]
f1_score_data = [0.8181818181818181, 0.8181818181818181, 0.8181818181818181, 0.8181818181818181, 0.8181818181818181, 0.8181818181818181, 0.8181818181818181, 0.8181818181818181, 0.8181818181818181, 0.8181818181818181] 

# Calculate the average for each metric
average_accuracy = sum(accuracy_data) / len(accuracy_data)
average_precision = sum(precision_data) / len(precision_data)
average_recall = sum(recall_data) / len(recall_data)
average_f1_score = sum(f1_score_data) / len(f1_score_data)

print("Average Accuracy:", average_accuracy)
print("Average Precision:", average_precision)
print("Average Recall:", average_recall)
print("Average F1 Score:", average_f1_score)

# Polynomial Data for Accuracy, Precision, Recall, and F1 Score across different algorithms
accuracy_data = [0.907608695652174, 0.907608695652174, 0.907608695652174, 0.907608695652174, 0.907608695652174, 0.907608695652174, 0.907608695652174, 0.907608695652174, 0.907608695652174, 0.907608695652174]
precision_data = [0.9411764705882353, 0.9411764705882353, 0.9411764705882353, 0.9411764705882353, 0.9411764705882353, 0.9411764705882353, 0.9411764705882353, 0.9411764705882353, 0.9411764705882353, 0.9411764705882353] 
recall_data = [0.897196261682243, 0.897196261682243, 0.897196261682243, 0.897196261682243, 0.897196261682243, 0.897196261682243, 0.897196261682243, 0.897196261682243, 0.897196261682243, 0.897196261682243]
f1_score_data = [0.9186602870813397, 0.9186602870813397, 0.9186602870813397, 0.9186602870813397, 0.9186602870813397, 0.9186602870813397, 0.9186602870813397, 0.9186602870813397, 0.9186602870813397, 0.9186602870813397] 

# Calculate the average for each metric
average_accuracy = sum(accuracy_data) / len(accuracy_data)
average_precision = sum(precision_data) / len(precision_data)
average_recall = sum(recall_data) / len(recall_data)
average_f1_score = sum(f1_score_data) / len(f1_score_data)

print("Average Accuracy:", average_accuracy)
print("Average Precision:", average_precision)
print("Average Recall:", average_recall)
print("Average F1 Score:", average_f1_score)

