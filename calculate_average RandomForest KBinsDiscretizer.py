# Data for Accuracy, Precision, Recall, and F1 Score across different algorithms
accuracy_data = [0.8695652173913043, 0.875, 0.9021739130434783, 0.8804347826086957, 0.8695652173913043, 0.8913043478260869, 0.8913043478260869, 0.8858695652173914, 0.8858695652173914, 0.8586956521739131]
precision_data = [0.9029126213592233, 0.9038461538461539, 0.9238095238095239, 0.9047619047619048, 0.8952380952380953, 0.8990825688073395, 0.9223300970873787, 0.9215686274509803, 0.9056603773584906,0.8932038834951457]
recall_data = [0.8691588785046729, 0.8785046728971962, 0.9065420560747663, 0.8878504672897196, 0.8785046728971962, 0.9158878504672897, 0.8878504672897196, 0.8785046728971962, 0.897196261682243, 0.8598130841121495]
f1_score_data = [0.8857142857142858, 0.8909952606635071, 0.9150943396226416, 0.8962264150943396, 0.8867924528301887, 0.9074074074074073, 0.9047619047619048, 0.8995215311004785, 0.9014084507042254, 0.8761904761904761]

# Calculate the average for each metric
average_accuracy = sum(accuracy_data) / len(accuracy_data)
average_precision = sum(precision_data) / len(precision_data)
average_recall = sum(recall_data) / len(recall_data)
average_f1_score = sum(f1_score_data) / len(f1_score_data)

print("Average Accuracy:", average_accuracy)
print("Average Precision:", average_precision)
print("Average Recall:", average_recall)
print("Average F1 Score:", average_f1_score)
